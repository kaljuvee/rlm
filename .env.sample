# =============================================================================
# RLM Examples - Environment Configuration
# =============================================================================
# Copy this file to .env and fill in API keys for the providers you want to use.
# You only need to configure the provider(s) you plan to use.
# =============================================================================

# --- OpenAI ---
OPENAI_API_KEY=sk-your-openai-api-key-here

# --- Anthropic ---
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# --- Google Gemini ---
GOOGLE_API_KEY=your-google-api-key-here

# --- Azure OpenAI ---
AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT=your-deployment-name
AZURE_OPENAI_API_VERSION=2024-02-01

# --- OpenRouter ---
OPENROUTER_API_KEY=sk-or-your-openrouter-api-key-here

# --- Portkey ---
PORTKEY_API_KEY=your-portkey-api-key-here

# --- LiteLLM (uses the provider keys above, or set a proxy) ---
# LITELLM_PROXY_BASE_URL=http://localhost:4000

# =============================================================================
# Default provider selection (used by examples that support --provider flag)
# Options: openai, anthropic, gemini, azure_openai, openrouter, portkey, litellm
# =============================================================================
RLM_DEFAULT_PROVIDER=openai

# =============================================================================
# Default model per provider (override if you prefer different models)
# =============================================================================
RLM_OPENAI_MODEL=gpt-4o-mini
RLM_ANTHROPIC_MODEL=claude-sonnet-4-20250514
RLM_GEMINI_MODEL=gemini-2.5-flash
RLM_AZURE_MODEL=gpt-4o-mini
RLM_OPENROUTER_MODEL=openai/gpt-4o-mini
RLM_PORTKEY_MODEL=gpt-4o-mini
RLM_LITELLM_MODEL=gpt-4o-mini
